{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando o tokenizador e a classe para testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tester import TokenizerTester\n",
    "from tatiktoken import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tester = TokenizerTester(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando o conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 15683.99it/s]\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"./corpus\"\n",
    "texts = tester.load_texts(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando o tokenizador com o conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [04:26<00:00, 37.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Teste concluído: 10000 textos processados\n",
      "Erros de decodificação: 0\n",
      "Tempo total: 267.93 segundos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tester.test_tokenizer(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados\n",
    "\n",
    "O tokenizador rodou conforme o esperado em todos os 10.000 textos que constituem o conjunto de dados. \n",
    "\n",
    "Ao rodar a método ``teste_tokenizer`` da classe ``TokenizerTester``, o processo de tokenização foi testado paralelamente. Isso garantiu maior eficiência no processamento dos textos, ao contribuir para uma execução rápida, considerando o volume de textos testados.\n",
    "\n",
    "Cada texto foi submetido a uma série de etapas, conforme detalhado abaixo:\n",
    "\n",
    "- Treinamento do Tokenizador:\n",
    "\t- O tokenizador foi treinado com base no texto original utilizando um vocabulário dinâmico. O tamanho do vocabulário foi adaptado ao número de caracteres únicos encontrados no texto, limitado a um máximo de 10.000 tokens.\n",
    "\n",
    "- Tokenização e Decodificação:\n",
    "\t- Cada texto foi tokenizado em uma sequência de IDs, e em seguida, esses IDs foram decodificados para reconstruir o texto original.\n",
    "\n",
    "- Verificação de Erros:\n",
    "\t- Para garantir a precisão do processo, foi realizada uma comparação entre o texto original e o texto decodificado. Caso houvesse qualquer divergência entre as duas versões, um erro de tipo 'diff' seria registrado.\n",
    "\n",
    "- Tratamento de Exceções:\n",
    "\t- Exceções durante a tokenização ou decodificação foram capturadas e registradas com a mensagem correspondente, sem interromper a execução geral dos testes.\n",
    "\n",
    "Caso tivesse ocorrido algum erro durante a execução dos testes, uma lista de tuplas no formato (id do documento, erro ocorrido) seria exibida logo após o relatório, permitindo fácil identificação e investigação dos problemas. Foram necessárias algumas iterações até alcançarmos uma lista de erros vazia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
